{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "# Local configuration\n",
    "# MODEL_DIR     = \"../models/vilt_ft\"\n",
    "# SRC_PATH      = \"../data/csvs/vqa.csv\"\n",
    "# IMAGE_DIR     = \"../data/curated_images\"\n",
    "# DEST_PATH     = \"../data/csvs/preds_vilt_ft.csv\"\n",
    "\n",
    "# Kaggle configuration\n",
    "MODEL_DIR     = \"../input/vrdata/data/models/vilt_ft\"\n",
    "SRC_PATH      = \"../input/vrdata/data/csvs/vqa.csv\"\n",
    "IMAGE_DIR     = \"../input/vrdata/data/curated_images\"\n",
    "DEST_PATH     = \"/kaggle/working/preds_vilt_ft.csv\"\n",
    "\n",
    "BASE_MODEL    = \"dandelin/vilt-b32-finetuned-vqa\"\n",
    "USE_FINETUNED = True\n",
    "SEED          = 7\n",
    "SAMPLE_SIZE   = 10000\n",
    "MAX_LENGTH    = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data loading\n",
    "df = pd.read_csv(SRC_PATH)\n",
    "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Model and processor\n",
    "if USE_FINETUNED:\n",
    "    print(f\"Loading finetuned with LoRA adapters from {MODEL_DIR}\")\n",
    "    processor = ViltProcessor.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "    base_model = ViltForQuestionAnswering.from_pretrained(BASE_MODEL)\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_DIR)\n",
    "else:\n",
    "    print(f\"Loading baseline ViLT from {BASE_MODEL}\")\n",
    "    processor = ViltProcessor.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    model     = ViltForQuestionAnswering.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Inference loop\n",
    "with open(DEST_PATH, mode=\"w\", newline=\"\", encoding=\"utf-8\") as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    writer.writerow([\"filename\", \"question\", \"answer\", \"prediction\"])\n",
    "\n",
    "    for fn, question, answer in tqdm(\n",
    "        df_sample[[\"filename\", \"question\", \"answer\"]].itertuples(index=False),\n",
    "        total=len(df_sample),\n",
    "        desc=\"Running ViLT VQA Inference\"\n",
    "    ):\n",
    "        img_path = os.path.join(IMAGE_DIR, fn)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(\n",
    "                images       = image,\n",
    "                text         = question,\n",
    "                truncation   = True,\n",
    "                max_length   = MAX_LENGTH,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(**inputs)\n",
    "                pred_id = out.logits.softmax(dim=1).argmax(dim=1).item()\n",
    "                pred = model.config.id2label[pred_id].strip().lower()\n",
    "\n",
    "        except Exception:\n",
    "            pred = \"\"\n",
    "\n",
    "        writer.writerow([fn, question, answer, pred])\n",
    "\n",
    "print(f\"Saved predictions to {DEST_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
