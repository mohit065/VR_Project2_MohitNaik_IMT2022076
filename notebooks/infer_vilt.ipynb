{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "# Config\n",
    "SRC_PATH     = \"../data/vqa.csv\"\n",
    "IMAGE_SRC_DIR = \"../data/curated_images\"\n",
    "DEST_PATH    = \"../data/predictions_vilt.csv\"\n",
    "MODEL_PATH = '../models/vilt_ft.pth'\n",
    "\n",
    "SEED        = 7\n",
    "SAMPLE_SIZE = 10000\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(SRC_PATH)\n",
    "df_sample = df.sample(n=SAMPLE_SIZE, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Initialize processor and model (Hugging Face checkpoint)\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# If a fine-tuned checkpoint exists locally, load it\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"Loading fine-tuned ViLT model from {MODEL_PATH}\")\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(state_dict)\n",
    "else:\n",
    "    print(\"No local ViLT checkpoint found. Using the pre-trained Hugging Face model.\")\n",
    "\n",
    "# Move model to device and set to eval mode\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Open output CSV\n",
    "with open(DEST_PATH, mode=\"w\", newline=\"\", encoding=\"utf-8\") as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    writer.writerow([\"filename\", \"question\", \"answer\", \"prediction\"])\n",
    "\n",
    "    # Inference loop\n",
    "    for row in tqdm(df_sample.itertuples(index=False), total=len(df_sample), desc=\"ViLT VQA Inference\", unit=\"it\"):\n",
    "        filename, question, answer = row.filename, row.question, row.answer\n",
    "        img_path = os.path.join(IMAGE_SRC_DIR, filename)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(images=img, text=question, return_tensors=\"pt\").to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                pred_id    = outputs.logits.argmax(-1).item()\n",
    "                prediction = model.config.id2label[pred_id]\n",
    "        except Exception as e:\n",
    "            prediction = \"\"\n",
    "        writer.writerow([filename, question, answer, prediction])\n",
    "\n",
    "print(f\"Saved predictions to {DEST_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
