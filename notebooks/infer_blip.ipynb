{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading finetuned with LoRA adapters from ../models/blip_ft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\vrenv\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LoraConfig.__init__() got an unexpected keyword argument 'eva_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m     processor \u001b[38;5;241m=\u001b[39m BlipProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_DIR, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m BlipForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL)\n\u001b[1;32m---> 38\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading baseline BLIP from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\vrenv\\lib\\site-packages\\peft\\peft_model.py:327\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[1;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# load the config\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     config \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m    328\u001b[0m         PeftConfig\u001b[38;5;241m.\u001b[39m_get_peft_type(\n\u001b[0;32m    329\u001b[0m             model_id,\n\u001b[0;32m    330\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubfolder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    331\u001b[0m             revision\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    332\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    333\u001b[0m             use_auth_token\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_auth_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    334\u001b[0m             token\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    335\u001b[0m         )\n\u001b[0;32m    336\u001b[0m     ]\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PeftConfig):\n\u001b[0;32m    338\u001b[0m     config\u001b[38;5;241m.\u001b[39minference_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_trainable\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\vrenv\\lib\\site-packages\\peft\\config.py:151\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[0;32m    150\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_peft_type(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\envs\\vrenv\\lib\\site-packages\\peft\\config.py:118\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_peft_type\u001b[1;34m(cls, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     config_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config_cls(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: LoraConfig.__init__() got an unexpected keyword argument 'eva_config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "# Local configuration\n",
    "# MODEL_DIR     = \"../models/blip_ft\"\n",
    "# SRC_PATH      = \"../data/csvs/vqa.csv\"\n",
    "# IMAGE_DIR     = \"../data/curated_images\"\n",
    "# DEST_PATH     = \"../data/csvs/preds_blip_ft.csv\"\n",
    "\n",
    "# Kaggle configuration\n",
    "MODEL_DIR     = \"../input/vrdata/data/models/blip_ft\"\n",
    "SRC_PATH      = \"../input/vrdata/data/csvs/vqa.csv\"\n",
    "IMAGE_DIR     = \"../input/vrdata/data/curated_images\"\n",
    "DEST_PATH     = \"/kaggle/working/preds_blip_ft.csv\"\n",
    "\n",
    "BASE_MODEL    = \"Salesforce/blip-vqa-base\"\n",
    "USE_FINETUNED = True\n",
    "SEED          = 7\n",
    "SAMPLE_SIZE   = 10000\n",
    "MAX_LENGTH    = 128\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Data loading\n",
    "df = pd.read_csv(SRC_PATH)\n",
    "df_sample = df.sample(n=min(SAMPLE_SIZE, len(df)), random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Model and processor\n",
    "if USE_FINETUNED:\n",
    "    print(f\"Loading finetuned with LoRA adapters from {MODEL_DIR}\")\n",
    "    processor = BlipProcessor.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "    base_model = BlipForQuestionAnswering.from_pretrained(BASE_MODEL)\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_DIR)\n",
    "else:\n",
    "    print(f\"Loading baseline BLIP from {BASE_MODEL}\")\n",
    "    processor = BlipProcessor.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    model = BlipForQuestionAnswering.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Inference loop\n",
    "with open(DEST_PATH, mode=\"w\", newline=\"\", encoding=\"utf-8\") as out_file:\n",
    "    writer = csv.writer(out_file)\n",
    "    writer.writerow([\"filename\", \"question\", \"answer\", \"prediction\"])\n",
    "\n",
    "    for fn, question, answer in tqdm(\n",
    "        df_sample[[\"filename\", \"question\", \"answer\"]].itertuples(index=False),\n",
    "        total=len(df_sample),\n",
    "        desc=\"Running BLIP VQA Inference\"\n",
    "    ):\n",
    "        img_path = os.path.join(IMAGE_DIR, fn)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(\n",
    "                images       = image,\n",
    "                text         = question,\n",
    "                truncation   = True,\n",
    "                max_length   = MAX_LENGTH,\n",
    "                return_tensors = \"pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens = 5,\n",
    "                    num_beams      = 1,\n",
    "                )\n",
    "\n",
    "            pred = processor.tokenizer.decode(\n",
    "                out_ids[0],\n",
    "                skip_special_tokens=True\n",
    "            ).strip().lower()\n",
    "\n",
    "        except Exception:\n",
    "            pred = \"\"\n",
    "\n",
    "        writer.writerow([fn, question, answer, pred])\n",
    "\n",
    "print(f\"Saved predictions to {DEST_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
