{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee386b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bert_score import score as bertscore_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering, get_scheduler\n",
    "\n",
    "# Configuration\n",
    "# SRC_PATH = '../data/csvs/vqa.csv'\n",
    "# IMAGE_DIR = '../data/curated_images'\n",
    "# DEST_DIR = '../data/models'\n",
    "\n",
    "# Kaggle configuration\n",
    "SRC_PATH = '../input/vrdata/data/csvs/vqa.csv'\n",
    "IMAGE_DIR = '../input/vrdata/data/curated_images'\n",
    "DEST_DIR = '/kaggle/working'\n",
    "\n",
    "MODEL_NAME      = 'dandelin/vilt-b32-finetuned-vqa'\n",
    "BATCH_SIZE      = 16\n",
    "EVAL_BATCH_SIZE = 32\n",
    "N_EPOCHS        = 3\n",
    "LEARNING_RATE   = 5e-5\n",
    "LORA_R          = 16\n",
    "LORA_ALPHA      = 32\n",
    "LORA_DROPOUT    = 0.1\n",
    "MAX_LENGTH      = 128\n",
    "WARMUP_STEPS    = 0\n",
    "\n",
    "# Accelerator setup\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "DEVICE = accelerator.device\n",
    "print(f\"Using {accelerator.state.num_processes} GPU(s), fp16\")\n",
    "\n",
    "# Dataset\n",
    "class ViltVQADataset(Dataset):\n",
    "    def __init__(self, df, image_dir, label2id):\n",
    "        self.image_dir = image_dir\n",
    "        self.entries = []\n",
    "        self.label2id = label2id\n",
    "        unk_id = label2id.get('other', label2id.get('unknown', 0))\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Verifying images\"):\n",
    "            img_path = os.path.join(image_dir, str(row['filename']))\n",
    "            ans = str(row['answer'])\n",
    "            if os.path.exists(img_path):\n",
    "                ans_id = label2id.get(ans, unk_id)\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                self.entries.append({\n",
    "                    \"image\": image,\n",
    "                    \"question\": str(row['question']),\n",
    "                    \"answer\": ans_id\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Missing {img_path}\")\n",
    "\n",
    "        if not self.entries:\n",
    "            raise RuntimeError('No valid entries found.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.entries[idx]\n",
    "\n",
    "# Collate function\n",
    "def vqa_collate_fn(batch):\n",
    "    images    = [item[\"image\"]    for item in batch]\n",
    "    questions = [item[\"question\"] for item in batch]\n",
    "    ans_ids   = [item[\"answer\"]   for item in batch]\n",
    "\n",
    "    enc = processor(\n",
    "        images=images,\n",
    "        text=questions,\n",
    "        truncation=True,\n",
    "        padding='longest',\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    batch_size = len(ans_ids)\n",
    "    num_labels = base_model.config.num_labels\n",
    "    labels = torch.zeros((batch_size, num_labels), dtype=torch.float)\n",
    "    for i, ans_id in enumerate(ans_ids):\n",
    "        labels[i, ans_id] = 1.0\n",
    "\n",
    "    enc['labels'] = labels\n",
    "    return {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "# Load model and processor\n",
    "print(f\"Loading {MODEL_NAME}…\")\n",
    "processor = ViltProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "base_model = ViltForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "print(f\"{MODEL_NAME} loaded successfully.\")\n",
    "\n",
    "label2id = base_model.config.label2id\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Apply LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(base_model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Prepare datasets and loaders\n",
    "df = pd.read_csv(SRC_PATH)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=7)\n",
    "\n",
    "train_ds = ViltVQADataset(train_df, IMAGE_DIR, label2id)\n",
    "val_ds   = ViltVQADataset(val_df,   IMAGE_DIR, label2id)\n",
    "print(f\"train={len(train_ds)}, val={len(val_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=vqa_collate_fn)\n",
    "eval_loader  = DataLoader(val_ds,   batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=vqa_collate_fn)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer   = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * N_EPOCHS\n",
    "scheduler   = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Accelerator preparation\n",
    "model, optimizer, train_loader, eval_loader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_loader, eval_loader, scheduler\n",
    ")\n",
    "accelerator.init_trackers(\"vilt-vqa\")\n",
    "\n",
    "# Finetuning\n",
    "print(\"Starting finetuning…\")\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_bar, total_loss = tqdm(train_loader, desc=f\"Epoch {epoch} train\"), 0.0\n",
    "    for step, batch in enumerate(train_bar, 1):\n",
    "        out  = model(**batch)\n",
    "        loss = out.loss\n",
    "        accelerator.backward(loss)\n",
    "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        train_bar.set_postfix(train_loss=total_loss/step)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for batch in tqdm(eval_loader, desc=f\"Epoch {epoch} eval\"):\n",
    "        with torch.no_grad():\n",
    "            out = model(**{k: batch[k] for k in ['input_ids', 'attention_mask', 'pixel_values']})\n",
    "            pred_ids = out.logits.argmax(dim=-1)\n",
    "            label_ids = batch['labels'].argmax(dim=-1)\n",
    "\n",
    "            preds.extend([id2label.get(i.item(), \"unknown\") for i in pred_ids])\n",
    "            refs.extend([id2label.get(i.item(), \"unknown\") for i in label_ids])\n",
    "\n",
    "    P, R, F1 = bertscore_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "    avg_f1 = F1.mean().item()\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        print(f\"\\nEpoch {epoch}: train_loss={total_loss/len(train_loader):.4f}, eval_bertscore_f1={avg_f1:.4f}\\n\")\n",
    "        ckpt = os.path.join(DEST_DIR, f\"vilt{epoch}\")\n",
    "        os.makedirs(ckpt, exist_ok=True)\n",
    "        model.save_pretrained(ckpt)\n",
    "        processor.save_pretrained(ckpt)\n",
    "        print(f\"Model saved to {ckpt}.\")\n",
    "\n",
    "print(\"Finetuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
