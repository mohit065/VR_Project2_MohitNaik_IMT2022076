{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bert_score import score as bertscore_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BlipProcessor,BlipForQuestionAnswering,get_scheduler\n",
    "\n",
    "# Local configuration\n",
    "# SRC_PATH = '../data/csvs/vqa.csv'\n",
    "# IMAGE_DIR = '../data/curated_images'\n",
    "# DEST_DIR = \"../data/csvs\"\n",
    "\n",
    "# Kaggle configuration\n",
    "SRC_PATH = '../input/vrdata/data/csvs/vqa.csv'\n",
    "IMAGE_DIR = '../input/vrdata/data/curated_images'\n",
    "DEST_DIR = \"/kaggle/working\"\n",
    "\n",
    "MODEL_NAME         = \"Salesforce/blip-vqa-base\"\n",
    "BATCH_SIZE         = 16\n",
    "EVAL_BATCH_SIZE    = 32\n",
    "N_EPOCHS           = 3\n",
    "LEARNING_RATE      = 5e-5\n",
    "LORA_R             = 16\n",
    "LORA_ALPHA         = 32\n",
    "LORA_DROPOUT       = 0.05\n",
    "MAX_LENGTH         = 128\n",
    "WARMUP_STEPS       = 0\n",
    "\n",
    "# Accelerator setup\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "DEVICE = accelerator.device\n",
    "print(f\"Using {accelerator.state.num_processes} GPU(s), fp16\")\n",
    "\n",
    "# Dataset\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(self, df, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.entries = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Verifying images\"):\n",
    "            img_path = os.path.join(image_dir, str(row['filename']))\n",
    "            if os.path.exists(img_path):\n",
    "                self.entries.append((img_path, str(row['question']), str(row['answer'])))\n",
    "            else:\n",
    "                print(f\"Warning: Missing {img_path}\")\n",
    "\n",
    "        if not self.entries:\n",
    "            raise RuntimeError(\"No valid images found.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, question, answer = self.entries[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        return {\"image\": image, \"question\": question, \"answer\": answer}\n",
    "\n",
    "# Collate function\n",
    "def vqa_collate_fn(batch):\n",
    "    images   = [item[\"image\"]    for item in batch]\n",
    "    questions= [item[\"question\"] for item in batch]\n",
    "    answers  = [item[\"answer\"]   for item in batch]\n",
    "    enc = processor(\n",
    "        images      = images,\n",
    "        text        = questions,\n",
    "        text_target = answers,\n",
    "        padding     = \"longest\",\n",
    "        truncation  = True,\n",
    "        max_length  = MAX_LENGTH,\n",
    "        return_tensors = \"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\":      enc.input_ids,\n",
    "        \"attention_mask\": enc.attention_mask,\n",
    "        \"pixel_values\":   enc.pixel_values,\n",
    "        \"labels\":         enc.labels\n",
    "    }\n",
    "\n",
    "# Load model and processor\n",
    "print(f\"Loading {MODEL_NAME}…\")\n",
    "model     = BlipForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "processor = BlipProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "print(f\"{MODEL_NAME} loaded successfully.\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r             = LORA_R,\n",
    "    lora_alpha    = LORA_ALPHA,\n",
    "    target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"query\",\"key\",\"value\"],\n",
    "    lora_dropout  = LORA_DROPOUT,\n",
    "    bias          = \"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Prepare datasets and loaders\n",
    "df = pd.read_csv(SRC_PATH)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=7)\n",
    "\n",
    "train_ds = VQADataset(train_df, IMAGE_DIR)\n",
    "val_ds   = VQADataset(val_df,   IMAGE_DIR)\n",
    "print(f\"train={len(train_ds)}, val={len(val_ds)}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=vqa_collate_fn)\n",
    "eval_loader  = DataLoader(val_ds,   batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=vqa_collate_fn)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer   = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * N_EPOCHS\n",
    "scheduler   = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Accelerator preparation\n",
    "model, optimizer, train_loader, eval_loader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_loader, eval_loader, scheduler\n",
    ")\n",
    "accelerator.init_trackers(\"vqa-lora\")\n",
    "\n",
    "# Finetuning\n",
    "print(\"Starting finetuning…\")\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_bar, total_loss = tqdm(train_loader, desc=f\"Epoch {epoch} train\"), 0.0\n",
    "    for step, batch in enumerate(train_bar, 1):\n",
    "        out   = model(**batch)\n",
    "        loss  = out.loss\n",
    "        accelerator.backward(loss)\n",
    "        accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        train_bar.set_postfix(train_loss=total_loss/step)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "    for batch in tqdm(eval_loader, desc=f\"Epoch {epoch} eval\"):\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                input_ids      = batch[\"input_ids\"],\n",
    "                attention_mask = batch[\"attention_mask\"],\n",
    "                pixel_values   = batch[\"pixel_values\"],\n",
    "                max_length     = MAX_LENGTH,\n",
    "                num_beams      = 1,\n",
    "                no_repeat_ngram_size = 2\n",
    "            )\n",
    "        preds.extend(processor.tokenizer.batch_decode(\n",
    "            gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "        refs.extend(processor.tokenizer.batch_decode(\n",
    "            batch[\"labels\"], skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "\n",
    "    P, R, F1 = bertscore_score(preds, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "    avg_f1 = F1.mean().item()\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        print(f\"\\nEpoch {epoch}: train_loss={total_loss/len(train_loader):.4f}, eval_bertscore_f1={avg_f1:.4f}\\n\")\n",
    "        ckpt = os.path.join(DEST_DIR, f\"blip{epoch}\")\n",
    "        os.makedirs(ckpt, exist_ok=True)\n",
    "        model.save_pretrained(ckpt)\n",
    "        processor.save_pretrained(ckpt)\n",
    "        print(f\"Model saved to {ckpt}.\")\n",
    "\n",
    "print(\"Finetuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
